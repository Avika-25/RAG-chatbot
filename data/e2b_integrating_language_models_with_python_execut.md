---
title: E2B: Integrating Language Models with Python Execution for Advanced Analytics
date: December 18, 2024
url: https://www.buildfastwithai.com/blogs/integrating-execution-environments-with-language-models-using-e2b-and-langchain
---

# E2B: Integrating Language Models with Python Execution for Advanced Analytics

## What is E2B?

## LangChain + Function Calling + E2B Integration

## Step 1: Install the Required Libraries

## Step 2: Import Libraries

## Step 3: Initialize the Code Interpreter

## Step 4: Set Up the Language Model

## Step 5: Create a Prompt Template

## Step 6: Chain the LLM with Code Execution

## Step 7: Execute a Task

## Step 8: Visualizing Data

## Conclusion

## Resources

### Why Use E2B?

### What is LangChain?

### Explanation of the Libraries:

### Detailed Breakdown of Imports:

### Why Use a Code Interpreter?

### Parameters Explained:

### Detailed Explanation:

### What is an LLMChain?

### Step-by-Step Breakdown:

### Expected Outcome:

#### Sample Output:

You’re not just reading about AI today — you’re about to build it.

Join Build Fast with AI’s Gen AI Launch Pad 2025—a 6-week transformation program designed to accelerate your AI mastery and empower you to build revolutionary applications.

E2B (Execution to Bot) is a powerful Python library designed to integrate execution environments with language models. It enables you to:

Traditional language models can generate code effectively, but executing that code safely within your environment can be challenging due to security risks or dependency conflicts. E2B addresses this by offering:

E2B allows developers and data scientists to combine language model capabilities with robust execution environments, making tasks like automated code generation, data analysis, and problem-solving easier and more efficient.

This notebook demonstrates how to integrate LangChain, function calling, and E2B to build an environment where a language model can:

LangChain is a framework designed for developing applications powered by large language models (LLMs). It allows developers to:

Combining LangChain with E2B enables a seamless workflow where language models not only generate code but also execute and refine it based on real-time results.

Let’s break this integration down step-by-step.

The first step is installing the necessary libraries for E2B and LangChain. Run the following command:

Ensure that your environment is set up correctly by verifying the installations.

After installation, import the necessary libraries to set up the execution environment:

Create an instance of the CodeInterpreter to enable code execution:

The CodeInterpreter provides a safe environment to run code generated by the language model. This prevents potential security risks and ensures the code runs in isolation.

Initialize the language model using LangChain’s OpenAI class:

Use PromptTemplate to create structured prompts that guide the language model in generating Python code:

You can customize this template to suit different coding scenarios or add constraints to refine the output.

Create an LLMChain to link the language model’s code generation with the execution step:

An LLMChain combines:

This chain allows you to repeatedly generate and execute code based on different tasks.

Now let’s execute a sample task. For example, let’s generate and run code to calculate the sum of the first 10 numbers:

The model generates and executes the code correctly, returning the expected result.

You can extend this to more complex tasks like plotting graphs. For instance, let’s visualize a quadratic function:

A plot of the function for values from -10 to 10, displayed within the notebook.

By integrating E2B, LangChain, and function calling, you can automate complex workflows, generate and execute code dynamically, and simplify tasks like data analysis and visualization.

---------------------------------

Stay Updated:- Follow Build Fast with AI pages for all the latest AI updates and resources.

Experts predict 2025 will be the defining year for Gen AI implementation.Want to be ahead of the curve?

Join Build Fast with AI’s Gen AI Launch Pad 2025 - your accelerated path to mastering AI tools and building revolutionary applications.

* Execute Python code in a secure, sandboxed environment.
* Generate Python code dynamically based on natural language.
* Automate workflows with the power of language models.
* Visualize data and conduct advanced analytics effortlessly.

* Chain together multiple LLM calls and tools to create complex workflows.
* Integrate external tools and APIs for enhanced capabilities.
* Create prompt templates and manage chains effectively.

* e2b_code_interpreter: Provides the core functionality for code execution within a sandboxed environment.
* langchain: A framework for developing applications powered by language models.
* langchainhub: Offers resources and templates for LangChain development.
* langchain-openai: Provides integrations with OpenAI’s language models.

* CodeInterpreter: This class handles the secure execution of code within E2B.
* OpenAI: This class from LangChain interfaces with OpenAI’s language models like GPT-3 or GPT-4.
* PromptTemplate: Helps create reusable templates for prompting the language model.
* LLMChain: Chains together prompts and language model calls for sequential tasks.

* Temperature: Controls the randomness of the output. A temperature of 0 makes the output deterministic, ensuring the same prompt yields the same result.

* input_variables: Defines placeholders for dynamic inputs, in this case, task.
* Template: The instruction given to the model, guiding it to generate Python code relevant to the task provided.

* E2B (Execution to Bot) Open Source GitHub Repo
* Build Fast With AI E2B (Execution to Bot) GitHub Repository
* OpenAI API Documentation

1. Sandboxed Execution: Run code in isolated environments to prevent harm to your system.
2. Real-Time Execution: Immediate feedback and results from executed code.
3. Error Handling: Detailed error messages to troubleshoot code issues.
4. Versatility: Useful for data analysis, visualization, automation, and more.

1. Generate Python code dynamically based on user input.
2. Execute that code in a secure sandbox.
3. Return the results for further analysis or visualization.

1. A language model (llm) that generates responses.
2. A prompt template that structures the input for the model.

1. Define the task: Specify the task in plain English.
2. Generate code: The language model produces Python code for the task.
3. Execute code: Run the generated code using code_interpreter.run().

```
CodeInterpreter
```

```
CodeInterpreter
```

```
OpenAI
```

```
0
```

```
PromptTemplate
```

```
input_variables
```

```
task
```

```
LLMChain
```

```
llm
```

```
code_interpreter.run()
```

